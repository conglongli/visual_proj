\begin{algorithm}[th!]
\small
    \caption{Strategy 1: distance accumulation}
    \SetKwInOut{Input}{input}
    \SetKwInOut{Output}{output}
    \Input{$n$ query images $\{Q_i\}$ with positive/negative flags $\{f_i\}$, \\
      $m$ dataset images $\{D_j\}$, \\
      cosine similarity between each $Q_i$ and $D_j$: $\{C_{ij}\}$, \\
      number of similar images $k$.}
    \Output{Top $k$ similar images.}
    $dist \leftarrow$ empty hashtable \\
    \For{\emph{each} $D_j$} {
        \If{$D_j \notin \{Q_i\}$} {
            \For{\emph{each} $Q_i$} {
                \eIf{$f_i$ is positive} {
                    $dist[D_j] \leftarrow dist[D_j] + C_{ij}$ \\
                } {
                    $dist[D_j] \leftarrow dist[D_j] - C_{ij}$ \\
                }
            }
        }
    }
    \Return top $k$ $D_j$ with the largest $dist[D_j]$
    \label{alg:accumulation}
\end{algorithm}

\section{Related Work}
\subsection{Content-Based Image Retrieval}
Content-based image retrieval aims at searching for similar images
through the analysis of image content. As DNNs learn rich mid-level
image descriptors, ImageNet uses the feature vectors from the 7th
layer in image retrieval and demonstrated outstanding
performance~\cite{krizhevsky2012imagenet}. Babenko \textit{et al.}
proposed to compress the DNN features using PAC and discriminative
dimensionality reduction to improve the efficiency~\cite{babenko2014neural}.

Due to the recent growth of visual contents, rapid search in a large
database becomes an emerging need. In stead of linear search which has
high computational cost, a practical strategy is to use the technique
of Approximate Nearest Neighbor (ANN) or hashing based method for
speed up~\cite{gionis1999similarity,weiss2009spectral,kulis2009learning,
norouzi2011minimal,liu2012supervised,xia2014supervised}. These methods
project the high-dimensional features to a lower dimensional space, and
then generate the compact binary codes. Benefiting from the produced
binary codes, fast image search can be carried out via binary pattern
matching or Hamming distance measurement. However, these methods require
to use similarity matrix to describe the relationship of the image pairs, which
is no practical for a large-scale dataset. Recently Lin \textit{et al.}
propose an effective deep learning framework to generate binary hash codes
and image representations in a point-wised manner, making it suitable
for large-scale datasets~\cite{lin2015deep}.

\subsection{Image Analysis Applications}
AverageExplorer is an interactive framework that allows an user to rapidly
explore and visualize a large image collection using the medium of average
images~\cite{zhu2014averageexplorer}. This real-time system provides a
way to summarize large amounts of visual data by weighted averages of
an image collection, with the weights reflecting user-indicated importance.
Zhu \textit{et al.} propose a way to help user-controlled realistic image
manipulation by learning the manifold of natural images and defining the
image editing operations with constraints lie on the learned manifold at
all times. Thus the model automatically adjusts the output keeping all edits
as realistic as possible~\cite{zhu2016generative}.

\section{Algorithm}
\label{sec:algo}

Previous work use Euclidean distance between the query image and dataset image
to find similar images~\cite{lin2015deep}. However, there are mainly two
remaining challenges when dealing with multiple query images. First, Euclidean
distances may vary a lot between a dataset image and different query images. We
need a "nomarlized Euclidean distances" to give uniform weight on different query
images.  On the other hand, Cosine similarity ($A \cdot B/||A||||B||$) always
ranges from -1 to 1 (higher means more similar). Thus we use cosine similarity
between the query image and dataset image to find similar images.

The second challenge is the strategy to find similar images for multiple query images.
Using the cosine similarity between each dataset image and query image, we need
to select the top similar images based on all positive and negative query images.
We propose two different algorithms to solve this problem.

\paragraph{Accumulation}
Algorithm~\ref{alg:accumulation} describes our first strategy which accumulates the
cosine similarity score for each dataset image. For each dataset image, we calculate
the total similarity score by incrementing/decrementing the similarity score for a
positive/negative query image, respectively. Since each single similarity score ranges
from -1 to 1 where higher means more similar, higher total similarity score means that
the dataset image is more similar to what the query images describe. At last we return
the top dataset images with the largest total similarity score.

\paragraph{Set union and difference}
Algorithm~\ref{alg:set} describes our second strategy which uses set union and difference
to find the similar images. First we find the top similar images for each query image
using the cosine similarity scores. At the same time we also calculate the accumulated
total similarity scores for those top images like the first strategy above.
Next we do set union for the top similar images for
all positive query images and all negative query images to build a positive set and a negative
set. At last we do a set difference between the two sets. We then return the top
dataset images from the final set with the largest total similarity score. We think this
strategy would produce more "stable" results since it exclude the case where a dataset image
might not be very similar to any positive query image (each single cosine similarity score) but
has a relatively high total similarity score.

\begin{algorithm}[t]
\small
    \caption{Strategy 2: set union and difference}
    \SetKwInOut{Input}{input}
    \SetKwInOut{Output}{output}
    \Input{$n$ query images $\{Q_i\}$ with positive/negative flags $\{f_i\}$, \\
      $m$ dataset images $\{D_j\}$, \\
      consine similarity between each $Q_i$ and $D_j$: $\{C_{ij}\}$, \\
      number of similar images $k$.}
    \Output{Top $k$ similar images.}
    $dist \leftarrow$ empty hashtable \\
    $p \leftarrow \emptyset$ \\
    $n \leftarrow \emptyset$ \\
    \For{\emph{each} $Q_i$} {
        $cand \leftarrow$ top $D_j$ with the largest $C_{ij}$ \\
        \For{\emph{each} $D_j$ in $cand$} {
            \eIf{$f_i$ is positive} {
                $dist[D_j] \leftarrow dist[D_j] + C_{ij}$ \\
            } {
                $dist[D_j] \leftarrow dist[D_j] - C_{ij}$ \\
            }
        }
        \eIf{$f_i$ is positive} {
            $p \leftarrow p \cup cand$ \\
        } {
            $n \leftarrow n \cup cand$ \\
        }   
    }
    $ret \leftarrow p \setminus n$ \\
    \Return top $k$ $D_j$ from $ret$ with the largest $dist[D_j]$
    \label{alg:set}
\end{algorithm}


